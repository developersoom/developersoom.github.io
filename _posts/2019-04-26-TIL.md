---
layout: post
title: 190426 TIL 프로젝트 - 크롤링 (puppeteer)

permalink: /til/:year/:month/:day/:title/
categories: [TIL (Today I Learned)]
comments: true
---

## **puppeteer**

### 크롤링을 우습게 본 것은 아주 큰 잘못이었으... 333

- 동적로딩도 크롤링 할 수 있는 라이브러리인 `puppeteer` 사용법을 공부했다.
- 예전엔 `phantomJS`를 많이 썼다고 하는데, 요샌 `puppeteer`를 쓰는 추세라고.. 
- `puppeteer`가 무겁다는 말도 있던데, 일단 새로 나온 라이브러리니 써보기로..!!

- [공식문서](https://github.com/GoogleChrome/puppeteer/blob/v1.14.0/docs/api.md#pageevaluatepagefunction-args)와 [The Definitive Guide to Web Scraping with NodeJs & Puppeteer](https://learnscraping.com/nodejs-web-scraping-with-puppeteer/)을 보고 따라했다. 


### **크롤링을 하자 크롤링 크롤크롤...**

- 우리 크롤링의 관건은 1차 크롤링을 한 후, 거기서 각각(약 160개)의 url에 접속해 각각 2차 크롤링을 해야하는 것이다.  

- 2차 크롤링은 동적로딩이 아니여서 기존에 했던 `cheerio`로 했다. 

- 2가지 문제가 발생했는데, 
    1) 비동기로 값을 가져오니 return 시점에서 array가 채워져 있지 않아서 빈 어레이를 뿜어댔다. (~~`await`를 덕지덕지 여기저기 붙여보았으나 실패~~)
    2) 160번 정도의 request를 하다보니 `TIMEOUT`에러가 났다. 
    [NodeJs Error: connect ETIMEDOUT](https://stackoverflow.com/questions/23575683/nodejs-error-connect-etimedout)

- 1번 문제는 `Promise.all`을 써서 해결하였다. (비동기 전문가에게 자문을 구했..)

- 2번 문제는 일단 지역을 `서울`로만 좁혀서 request를 보냈다. (점차 늘려가는 방향으로...)

![image](https://user-images.githubusercontent.com/40848630/56851314-8c110c00-6948-11e9-87ba-40a14ae98f54.png)


## **짜잔-! 눈물의 크롤링... ㅠㅠ**

- 정말 `첩첩산중`이라는 말이 적절했던 크롤링...
- 크롤링만 해결하면 되겠지 했는데... 휴.
- 지금도 일단 크롤링은 했지만 이 데이터를 어떤 형태로 저장하고 정리해서 가져오는 게 효율적일지 고민해봐야되고, 클라이언트에서 어떻게 보여줄건지도 고민해야되고... 저 값을 db에 보관할 것인지 그냥 값으로(?) 넘겨줄 것인지도 고민해봐야되고, 크롤링은 하루에 한번만 할 것인지, 일주일에 한번만 할 것인지도 고민해봐야되고.... (이쯤되면 그냥 고민을 하기 위해 코딩을 하는 것이 아닐지...)
- 그래도 크롤링 하면서 느낀건 **하면 된다!** 는 것..!! 다들 포기하라 했지만... 난 누가 포기하라그러면 괜히 오기생겨서 더 하게 되더라...ㅎㅎㅎ 뿌듯하면서 피곤한 크롤링 한 주였다. 휴~~ 그래도 재밌었당!! 
